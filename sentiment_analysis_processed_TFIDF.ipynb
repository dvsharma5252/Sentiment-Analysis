{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"sentiment_analysis_processed_TFIDF.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"EtrPd-6ocwHB"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BdM5tsEcwHE"},"source":["f = open('tr')\n","raw = f.read().splitlines()\n","f.close()\n","print(raw[0])\n","print(raw[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuwCYs9WcwHF"},"source":["text = ''\n","\n","for i in raw:\n","    text += re.sub('[^a-z ]', '', i.strip()[2:].lower())+' '\n","    \n","print(text)\n","\n","word = list(set(nltk.word_tokenize(text)))\n","print(len(word))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ADcV4YRDcwHF"},"source":["filtered = list(set([w for w in word if not w in stopwords.words('english')]))\n","print(len(filtered))\n","print(filtered)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ZiV9OoUcwHF"},"source":["wordnet_lemmatizer = WordNetLemmatizer()\n","\n","wordnet_lemmatized_v = []\n","\n","for item in filtered:\n","    wordnet_lemmatized_v.append(wordnet_lemmatizer.lemmatize(item, 'v'))\n","\n","wordnet_v = list(set(wordnet_lemmatized_v))\n","print(len(wordnet_v))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBt0W3nccwHG"},"source":["train_file_wordnet_v = []\n","Y_train_raw = []\n","\n","for line in raw:\n","    Y_train_raw.append(int(line[0]))\n","    x = re.sub('[^a-z ]', '', line.strip()[2:].lower())\n","    tokens = nltk.word_tokenize(x)\n","    filtered_tokens = [w for w in tokens if not w in stopwords.words('english')]\n","    stemmed_tokens = ''\n","    for item in filtered_tokens:\n","        stemmed_tokens += wordnet_lemmatizer.lemmatize(item, 'v')+' '\n","    train_file_wordnet_v.append(stemmed_tokens[:-1])\n","\n","print(len(train_file_wordnet_v))\n","print(train_file_wordnet_v[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUZyh0mqooE9"},"source":["vectorizer = TfidfVectorizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rl1jyHDEov2F"},"source":["X_train_tfidf = vectorizer.fit_transform(train_file_wordnet_v)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oPPUC-5xfxUf"},"source":["print(vectorizer.get_feature_names())\n","print(X_train_tfidf.shape)\n","print(type(X_train_tfidf))\n","X_train = X_train_tfidf.toarray()\n","Y_train = np.array(Y_train_raw)\n","print(type(X_train))\n","print(X_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsVfeegVcwHG"},"source":["test_file_wordnet_v = []\n","Y_test_raw = []\n","\n","f = open('te')\n","read_test = f.read().splitlines()\n","f.close()\n","\n","for line in read_test:\n","    Y_test_raw.append(int(line[0]))\n","    x = re.sub('[^a-z ]', '', line.strip()[2:].lower())\n","    tokens = nltk.word_tokenize(x)\n","    filtered_tokens = [w for w in tokens if not w in stopwords.words('english')]\n","    stemmed_tokens = ''\n","    for item in filtered_tokens:\n","        stemmed_tokens += wordnet_lemmatizer.lemmatize(item, 'v')+' '\n","    test_file_wordnet_v.append(stemmed_tokens[:-1])\n","\n","print(len(test_file_wordnet_v))\n","print(test_file_wordnet_v[4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4VZk1vgfj_t"},"source":["X_test_tfidf = vectorizer.transform(test_file_wordnet_v)\n","print(X_test_tfidf.shape)\n","print(type(X_test_tfidf))\n","X_test = X_test_tfidf.toarray()\n","Y_test = np.array(Y_test_raw)\n","print(type(X_test))\n","print(X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjZrBEyZfkCj"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import GridSearchCV"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKQ2VsOAfkF4"},"source":["model_parameters = {\n","        'n_estimators':[50, 100, 150, 200],\n","        'criterion':['gini', 'entropy'],\n","        'max_depth': [3, 5, 7, 9, 11]\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGmcw7uOgmqQ"},"source":["model = RandomForestClassifier(random_state=1)\n","gscv = GridSearchCV(estimator=model, \n","                    param_grid=model_parameters, \n","                    cv=5, \n","                    verbose=1, \n","                    n_jobs=-1,\n","                    scoring='roc_auc')\n","\n","gscv.fit(X_train, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVDWP00wgm2t"},"source":["print('The best parameter are -', gscv.best_params_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RfF00u-RgnB4"},"source":["print(gscv.best_score_)\n","print(gscv.best_estimator_)\n","print(gscv.scorer_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsUdLmsIkFMh"},"source":["print('AUC on test by gscv =', roc_auc_score(y_true=Y_test,\n","                                                        y_score=gscv.predict_proba(X_test)[:, 1]))"],"execution_count":null,"outputs":[]}]}